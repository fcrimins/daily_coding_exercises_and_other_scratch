{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark._ // this might obviate the need for `import sqlContext.implicits._` below\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://github.com/alexarchambault/jupyter-scala\n",
    "import $ivy.`org.slf4j:slf4j-nop:1.7.12`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.0-RC3`\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import jupyter.spark._ // this might obviate the need for `import sqlContext.implicits._` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/fred/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-nop/1.7.12/slf4j-nop-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/fred/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.helpers.NOPLoggerFactory]\n",
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@1e5fccd5\n",
       "\u001b[36msc\u001b[39m: \u001b[32mJupyterSparkContext\u001b[39m = jupyter.spark.JupyterSparkContext@746d16bc\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@3227a364\n",
       "\u001b[36mres1_3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"2.1.0\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@transient val sparkConf = new SparkConf().setAppName(\"20170216_Spark_MLLib_Recommender_Scala\").setMaster(\"local[2]\")\n",
    "@transient val sc = new JupyterSparkContext(sparkConf)\n",
    "val sqlContext = new SQLContext(sc)\n",
    "//import sqlContext.implicits._ // this causes an error in the next frame:\n",
    "sc.version                       // \"cmd2.sc:190: object Helper is not a member of package $sess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdatasets_path\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/fred/data/amazon\"\u001b[39m\n",
       "\u001b[36msmall_ratings_file\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/fred/data/amazon/ratings_Baby.csv\"\u001b[39m\n",
       "\u001b[36msmall_ratings_raw_data\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = /home/fred/data/amazon/ratings_Baby.csv MapPartitionsRDD[1] at textFile at cmd2.sc:4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load the raw ratings data\n",
    "val datasets_path = \"/home/fred/data/amazon\"\n",
    "// String joinedPath = new File(path1, path2).toString();\n",
    "val small_ratings_file = new java.io.File(datasets_path, \"ratings_Baby.csv\").toString\n",
    "val small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
    "//val small_ratings_raw_data_header = small_ratings_raw_data.take(1)(0) // ain't no header in my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// w/out this the following error: \"cmd3.sc:15: value toDF is not a member of org.apache.spark.rdd.RDD[Helper.this.Rating]\"\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRating\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mRating\u001b[39m\n",
       "\u001b[36msmall_ratings_data\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRating\u001b[39m] = [user: int, item: int ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// using default 'user' and 'item' column names as used by ALS.getUserCol and getItemCol\n",
    "case class Rating(user: Int, item: Int, rating: Double)\n",
    "//def str2int(s: String): Int = { math.abs(s.hashCode) % math.pow(10, 8).toInt }\n",
    "object Rating {\n",
    "    def apply(tokens: Array[String]) = new Rating(tokens(0).hashCode, tokens(1).hashCode, tokens(2).toDouble)\n",
    "}\n",
    "\n",
    "// see http://stackoverflow.com/questions/34964565/how-to-create-a-dataset-from-custom-class-person\n",
    "// for why this is necessary (see 'Spark Notebook' comment)\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "// parse the raw data into a new RDD\n",
    "val small_ratings_data = small_ratings_raw_data\n",
    "    //.filter((line: String) => line != small_ratings_raw_data_header)\n",
    "    .map(_.split(\",\"))\n",
    "    .map(Rating(_)).toDF.as[Rating].cache\n",
    "\n",
    "// TODO: implement implicit preferences perhaps? like here:\n",
    "// https://github.com/eBay/Spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRating\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mRating\u001b[39m(\u001b[32m-1613580310\u001b[39m, \u001b[32m-725463517\u001b[39m, \u001b[32m5.0\u001b[39m),\n",
       "  \u001b[33mRating\u001b[39m(\u001b[32m1185180308\u001b[39m, \u001b[32m-725463263\u001b[39m, \u001b[32m5.0\u001b[39m),\n",
       "  \u001b[33mRating\u001b[39m(\u001b[32m1941063073\u001b[39m, \u001b[32m-725461590\u001b[39m, \u001b[32m4.0\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// for illustrative purposes, take the first few lines of our RDD to see the result\n",
    "small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnratings\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m915446L\u001b[39m\n",
       "\u001b[36mnusers\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m531854L\u001b[39m\n",
       "\u001b[36mnitems\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m64391L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nratings = small_ratings_data.count() // 915446\n",
    "val nusers = small_ratings_data.map(_.user).distinct().count() // 531854\n",
    "val nitems = small_ratings_data.map(_.item).distinct().count() // 64391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before user filter dataset counts (training, validation, test) = (548570, 183503, 183373)\n",
      "After user filter dataset counts (training, validation, test) = (548570, 87141, 86807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valdation set size reduced by 53%\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|                user|                item|            rating|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|              548570|              548570|            548570|\n",
      "|   mean|  -6969681.643890479|-3.856756676566637E8| 4.116803689592942|\n",
      "| stddev|1.2403181335590982E9|1.2912695594648366E9|1.2901576455818002|\n",
      "|    min|         -2147446657|         -2147455739|               1.0|\n",
      "|    max|          2147479032|          2147211727|               5.0|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+-----------------+\n",
      "|summary|               user|                item|           rating|\n",
      "+-------+-------------------+--------------------+-----------------+\n",
      "|  count|              87141|               87141|            87141|\n",
      "|   mean| -8592992.062496414|-3.471510736339151E8|4.178756268576215|\n",
      "| stddev|1.241249516808962E9|1.2872958496973834E9|1.203833071805968|\n",
      "|    min|        -2147402751|         -2147455739|              1.0|\n",
      "|    max|         2147354236|          2146960956|              5.0|\n",
      "+-------+-------------------+--------------------+-----------------+\n",
      "\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|                user|                item|            rating|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|               86807|               86807|             86807|\n",
      "|   mean|  -8931683.362678126|-3.54418087480617...| 4.182911516352368|\n",
      "| stddev|1.2460274545304818E9|1.2874536955991986E9|1.2024890423826682|\n",
      "|    min|         -2147412835|         -2147455739|               1.0|\n",
      "|    max|          2147347275|          2147048041|               5.0|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtraining\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRating\u001b[39m] = [user: int, item: int ... 1 more field]\n",
       "\u001b[36mvalidation_\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRating\u001b[39m] = [user: int, item: int ... 1 more field]\n",
       "\u001b[36mtest_\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRating\u001b[39m] = [user: int, item: int ... 1 more field]\n",
       "\u001b[36mtrusers_\u001b[39m: \u001b[32mSet\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mSet\u001b[39m(\n",
       "  \u001b[32m-1106602373\u001b[39m,\n",
       "  \u001b[32m-1959073589\u001b[39m,\n",
       "  \u001b[32m1913896141\u001b[39m,\n",
       "  \u001b[32m-1686757815\u001b[39m,\n",
       "  \u001b[32m-912333438\u001b[39m,\n",
       "  \u001b[32m-1753452537\u001b[39m,\n",
       "  \u001b[32m930611718\u001b[39m,\n",
       "  \u001b[32m414560163\u001b[39m,\n",
       "  \u001b[32m-1645973862\u001b[39m,\n",
       "  \u001b[32m-1892452500\u001b[39m,\n",
       "  \u001b[32m-1492970957\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mtritems_\u001b[39m: \u001b[32mSet\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mSet\u001b[39m(\n",
       "  \u001b[32m-1792610910\u001b[39m,\n",
       "  \u001b[32m2136598920\u001b[39m,\n",
       "  \u001b[32m-1797079704\u001b[39m,\n",
       "  \u001b[32m1349788168\u001b[39m,\n",
       "  \u001b[32m-2110032324\u001b[39m,\n",
       "  \u001b[32m-1411554330\u001b[39m,\n",
       "  \u001b[32m-174974075\u001b[39m,\n",
       "  \u001b[32m-510289111\u001b[39m,\n",
       "  \u001b[32m-1427517807\u001b[39m,\n",
       "  \u001b[32m-1212341624\u001b[39m,\n",
       "  \u001b[32m691349445\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrusers\u001b[39m: \u001b[32mbroadcast\u001b[39m.\u001b[32mBroadcast\u001b[39m[\u001b[32mSet\u001b[39m[\u001b[32mInt\u001b[39m]] = Broadcast(18)\n",
       "\u001b[36mtritems\u001b[39m: \u001b[32mbroadcast\u001b[39m.\u001b[32mBroadcast\u001b[39m[\u001b[32mSet\u001b[39m[\u001b[32mInt\u001b[39m]] = Broadcast(19)\n",
       "\u001b[36mvalidation\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRating\u001b[39m] = [user: int, item: int ... 1 more field]\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRating\u001b[39m] = [user: int, item: int ... 1 more field]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// split it into train, validation, and test datasets\n",
    "val Array(training, validation_, test_) = small_ratings_data.randomSplit(Array(0.6, 0.2, 0.2), seed=0)\n",
    "println(s\"Before user filter dataset counts (training, validation, test) = (${training.count()}, ${validation_.count()}, ${test_.count()})\")\n",
    "\n",
    "// remove users from validation and test datasets that aren't in training, else suffer NaNs\n",
    "// per here: https://issues.apache.org/jira/browse/SPARK-14489\n",
    "val trusers_ = training.map(_.user).collect.toSet\n",
    "val tritems_ = training.map(_.item).collect.toSet\n",
    "val trusers = sc.broadcast(trusers_) // http://stackoverflow.com/questions/26214112/filter-based-on-another-rdd-in-spark\n",
    "val tritems = sc.broadcast(tritems_)\n",
    "val validation = validation_.filter(x => trusers.value.contains(x.user) && tritems.value.contains(x.item))\n",
    "val test = test_.filter(x => trusers.value.contains(x.user) && tritems.value.contains(x.item))\n",
    "println(s\"After user filter dataset counts (training, validation, test) = (${training.count()}, ${validation.count()}, ${test.count()})\")\n",
    "println(s\"Valdation set size reduced by ${100 - 100 * validation.count() / validation_.count()}%\")\n",
    "training.describe().show\n",
    "validation.describe().show\n",
    "test.describe().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rp/rank (0.3,18) the training RMSE is 0.4077053116456953 and validation RMSE is 2.13740144296016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rp/rank (0.3,22) the training RMSE is 0.4060075868931709 and validation RMSE is 2.1138878886870676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rp/rank (0.6,18) the training RMSE is 0.7303739363868421 and validation RMSE is 2.09519140800299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rp/rank (0.6,22) the training RMSE is 0.7303515297098322 and validation RMSE is 2.0666486100990786\n",
      "The best model was trained with rp/rank (0.6,22,als_5ae04a39b092)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "// old API: https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS\n",
       "// new API: https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.recommendation.ALS\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mseed\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5\u001b[39m\n",
       "\u001b[36mniters\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mregularization_parameters\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m0.3\u001b[39m, \u001b[32m0.6\u001b[39m)\n",
       "\u001b[36mranks\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m18\u001b[39m, \u001b[32m22\u001b[39m)\n",
       "\u001b[36mtolerance\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.02\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrmse\u001b[39m\n",
       "\u001b[36mminerr\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m2.0666486100990786\u001b[39m\n",
       "\u001b[36mbest\u001b[39m: (\u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mml\u001b[39m.\u001b[32mrecommendation\u001b[39m.\u001b[32mALSModel\u001b[39m) = (\u001b[32m0.6\u001b[39m, \u001b[32m22\u001b[39m, als_5ae04a39b092)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// proceed with the training phase\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "// old API: https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS\n",
    "// new API: https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.ml.recommendation.ALS\n",
    "\n",
    "val seed = 5\n",
    "val niters = 10\n",
    "val regularization_parameters = Array(0.3, 0.6)\n",
    "val ranks = Array(18, 22)\n",
    "val tolerance = 0.02\n",
    "\n",
    "// https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "def rmse(model: ALSModel, data: Dataset[Rating]) = {\n",
    "    val predictions = model.transform(data)\n",
    "    val evaluator = new RegressionEvaluator()\n",
    "        .setMetricName(\"rmse\")\n",
    "        .setLabelCol(\"rating\")\n",
    "        .setPredictionCol(\"prediction\")\n",
    "    val error = evaluator.evaluate(predictions)\n",
    "    (error, predictions)\n",
    "}\n",
    "\n",
    "var minerr = Double.PositiveInfinity\n",
    "var best: (Double, Int, ALSModel) = (-1.0, -1, null) // \"class Helper needs to be abstract, since variable best is not defined\"\n",
    "for(rp <- regularization_parameters; rank <- ranks) {\n",
    "    \n",
    "    val als = new ALS()\n",
    "        .setRank(rank) // number of latent factors in the model\n",
    "        .setMaxIter(niters) // formerly setIterations\n",
    "        .setRegParam(rp) // formerly setLambda (lambda specifies the regularization parameter in ALS)\n",
    "        .setImplicitPrefs(false) // implicit or explicit recommendations?\n",
    "        //.setNumUserBlocks(-1) // IllegalArgumentException: als_2b89f15f5bde parameter numUserBlocks given invalid value -1\n",
    "        //.setNumItemBlocks(-1) // IllegalArgumentException: als_e9bc5c270983 parameter numItemBlocks given invalid value -1\n",
    "    \n",
    "    val model = als.fit(training)\n",
    "    \n",
    "    val terr = rmse(model, training)._1\n",
    "    val verr = rmse(model, validation)._1\n",
    "    println(s\"For rp/rank ${(rp, rank)} the training RMSE is $terr and validation RMSE is $verr\")\n",
    "    if (verr < minerr) {\n",
    "        minerr = verr\n",
    "        best = (rp, rank, model)\n",
    "    }\n",
    "}\n",
    "\n",
    "println(s\"The best model was trained with rp/rank $best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 513:====================================================>(199 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 2.070002363103332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36merr\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m2.070002363103332\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val err = rmse(best._3, test)._1\n",
    "println(s\"For testing data the RMSE is $err\") // 2.0700"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
